{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "119da637-425b-4c5a-9c00-45fd1e2c50a4",
   "metadata": {},
   "source": [
    "## CSV files 3: reading larger-than-memory CSV files in batches\n",
    "By the end of this lecture you will be able to:\n",
    "- read larger-than-memory datasets with batching\n",
    "\n",
    "To work with larger-than-memory datasets we must:\n",
    "- process the dataset in chunks\n",
    "- combine the chunks into a single output\n",
    "\n",
    "We refer to each chunk of a dataset as a *batch*. We can read CSV files in batches in Polars.\n",
    "\n",
    "In the coming lectures we see how to process larger-than-memory datasets using *streaming*. Streaming better as Polars takes care of the batching and has algorithms to combine the chunks correctly for many operations such as groupbys and joins.\n",
    "\n",
    "We cover manual batching in this lecture to allow you to:\n",
    "- understand how Polars carries out streaming underneath the hood\n",
    "- create your own custom batching algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7cab59-f01d-49f5-81c1-65c0ec6f0d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "pl.Config.set_tbl_rows(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade891cd-98cd-4556-b46f-5eea77fed505",
   "metadata": {},
   "source": [
    "Although batching is for large datasets we can still do it with a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959942c6-7050-487b-93bf-12e6c1bb1eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = \"../data/titanic.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb3a6c9-7a84-4f04-9eb6-c3eb32d702a4",
   "metadata": {},
   "source": [
    "## Batched reader\n",
    "We read a CSV in batches by calling `pl.read_csv_batched` with the path of the CSV file. We tell Polars how many lines we want each batch `DataFrame` to be with the `batch_size` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b897e-35e3-44a5-a374-1038c699ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pl.read_csv_batched(csvFile,batch_size=10)\n",
    "reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d67f-281e-4eef-8245-866ac906b4a5",
   "metadata": {},
   "source": [
    "The `pl.read_csv_batched` function accept all the standard arguments for CSV processing such as setting delimiters or changing column names\n",
    "\n",
    "At this stage not much work has been done:\n",
    "- Polars has opened the CSV file\n",
    "- Polars has calculated some statistics to estimate the length of each line\n",
    "\n",
    "We can extract some batches from the CSV by calling `next_batch` on the `reader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6675ea-1aa0-44ae-a4e0-ddebd4793850",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pl.read_csv_batched(csvFile,batch_size=10)\n",
    "batches = reader.next_batches(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2669c0d-0417-4d13-aa40-bfd533d74df2",
   "metadata": {},
   "source": [
    "The output of `reader.next_batches` is a `list` of `DataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32960d2-6cea-4548-aed9-9ac56276dd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9a4a65-94ac-4abd-a23f-1ecfdd83176d",
   "metadata": {},
   "source": [
    "We inspect the first `DataFrame` in the `list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e2661e-e173-4175-8abd-6328fd21cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36428c24-a474-4463-b3ac-686510f41df0",
   "metadata": {},
   "source": [
    "We set `batch_size = 10` so we wanted each `DataFrame` to have 10 rows, but this first batch has 74 rows!\n",
    "\n",
    "The number of rows in each batch is not guaranteed to equal the `batch_size` argument. This is because with a CSV Polars has to estimate how large a batch will be in bytes before reading it.\n",
    "\n",
    "### Estimating batch size\n",
    "When Polars opens a CSV file it cannot know:\n",
    "- how many lines there are in a file\n",
    "- where each new line starts\n",
    "\n",
    "As such it cannot know exactly how many bytes to read to get 10 lines.\n",
    "\n",
    "Polars makes an estimate by first reading a sample of lines to get the mean and standard deviation of their length in bytes. It uses this to estimate the total number of bytes per line.\n",
    "\n",
    "If a CSV has mainly numerical or datetime data then the number of bytes per row will be very consistent and the actual batch size will closely match `batch_size`.\n",
    "\n",
    "However if a CSV has text data with variable length then the number of bytes per row will vary considerable and the actual batch size will differ from `batch_size`.\n",
    "\n",
    "Typically the relative difference between the actual batch size and `batch_size` will be smaller for larger datasets. A small dataset with variable strings like Titanic is the most challenging case.\n",
    "\n",
    "## Processing batches\n",
    "If we keep calling `reader.next_batches` it eventually returns a `NoneType` instead of a `list` when it has gone through all the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4793ac-437f-4ea8-9e94-d3ddf8e3b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pl.read_csv_batched(csvFile,batch_size=10)\n",
    "batches0 = reader.next_batches(5)\n",
    "batches1 = reader.next_batches(5)\n",
    "batches2 = reader.next_batches(5)\n",
    "batches3 = reader.next_batches(5)\n",
    "[type(batches0),type(batches1),type(batches2),type(batches3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ae0cc4-d0d0-49a2-aa15-25827b7c4542",
   "metadata": {},
   "source": [
    "On the last call of `reader.next_batches` the number of `DataFrames` in the list may be smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f80eb-bfe9-4dd6-a49b-34738a35d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pl.read_csv_batched(csvFile,batch_size=10)\n",
    "batches0 = reader.next_batches(5)\n",
    "batches1 = reader.next_batches(5)\n",
    "batches2 = reader.next_batches(5)\n",
    "[len(batches0),len(batches1),len(batches2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123a929d-a45d-41e9-a5d4-a7f077fc92de",
   "metadata": {},
   "source": [
    "## Custom batched algorithm\n",
    "We do a simple algorithm on a batched CSV to get the sum of the floating point columns. To do this we:\n",
    "1. create a `reader` by calling `pl.read_csv_batched`\n",
    "2. get batches of 5 `DataFrames` from `reader` at a time\n",
    "3. get the sum of the floating point columns for each `DataFrame`\n",
    "4. get the sum of the floating point columns for the batch of 5 `DataFrames`\n",
    "5. get the sum of the floating point columns for all the batches\n",
    "\n",
    "We pre-define the following function to do steps 3 (inside the list comprehension) and 4 (on `pl.concat`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945196a-a6f0-4668-a976-dc23d794a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumBatch(batch:list):\n",
    "    return (\n",
    "    # Step 4\n",
    "    pl.concat(\n",
    "        # Step 3\n",
    "        [\n",
    "            (\n",
    "                df\n",
    "                .select(\n",
    "                    pl.col(pl.Float64)\n",
    "                )\n",
    "                .sum()\n",
    "            ) for df in batch\n",
    "        ]\n",
    "    ).sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296505e5-01f6-41f1-a17b-14ba230c0c31",
   "metadata": {},
   "source": [
    "We now process the all of the batches in the cell below.\n",
    "\n",
    "We do step 1 to create `reader`\n",
    "\n",
    "We do step 2 in a `while` loop that stops when `reader.next_batches` returns a `NoneType`\n",
    "\n",
    "We do step 5 by calling `pl.concat().sum()` at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f18c1-ba88-431f-b695-85cd88046b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "reader = pl.read_csv_batched(csvFile,batch_size=5)\n",
    "proceed = True\n",
    "df_list = []\n",
    "# Step 2\n",
    "while proceed:\n",
    "    batch = reader.next_batches(3)\n",
    "    if not isinstance(batch,list):\n",
    "        proceed = False\n",
    "    else:\n",
    "        df_list.append(sumBatch(batch=batch))\n",
    "# Step 5\n",
    "(\n",
    "    pl.concat(df_list)\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5456e050-17b5-41d6-932b-f510c7454806",
   "metadata": {},
   "source": [
    "This example shows why it is nice that Polars can do this with built-in streaming functions for many operations! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271e6936-73c6-47cc-afb3-c6b0790cea6c",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "In the exercises you will develop your understanding of:\n",
    "- reading a CSV in batches\n",
    "- developing a batched algorithm\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Get the average of the `Age` and `Fare` columns by batch processing the CSV file.\n",
    "\n",
    "This is a trickier exercise. If you want a challenge you can implement it yourself, otherwise you can use the step-by-step approach below.\n",
    "\n",
    "Compare your answer with this non-batched version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ff77c-b82d-4c98-958e-8903896e5d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_csv(csvFile)\n",
    "    .select([\"Age\",\"Fare\"])\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ff29e-8437-44f8-9ee4-7a82032edf75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93b2c15d-c436-4274-9178-92e5e1a3405c",
   "metadata": {},
   "source": [
    "#### Step-by-step approach\n",
    "\n",
    "Re-use the algorithm above to get the **sum** of columns using a batched approach. Rename `df_list` to `df_sum_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957547d7-2a24-4a59-a08a-06debb1fbd7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6256b532-b4d0-4ac4-b300-1b5997b21935",
   "metadata": {},
   "source": [
    "Add a function called `countBatch` that counts the number of rows in each batch. Only the rows without `null` values should be counted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c16f0b9-2447-42ba-bc2e-2b30b3b71508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0aa2c8c-5210-4240-907d-2443d5ccdc0f",
   "metadata": {},
   "source": [
    "Get the sum of all the batches in `df_sum_list` and the total count of all the batches in `df_count_list`. Concatenate all the batched to get the total sum and the total count of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7231b5-2b57-4d6d-954c-dd2e6ce18726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cff6913f-ee2e-4e07-b288-736607636ec6",
   "metadata": {},
   "source": [
    "Divide the sum of all the batches by the total count of all the batches - compare with the non-batched solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc430ab-a727-4c7a-bbd3-66f439437da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e563ec25-d285-485e-a1ce-83adf9bdabc1",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Solution to exercise 1\n",
    "Re-use the algorithm above to get the sum of columns using a batched approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c10ff2c-f086-40f7-a319-9dede1ea1939",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = pl.read_csv_batched(csvFile,batch_size=5)\n",
    "proceed = True\n",
    "df_sum_list = []\n",
    "while proceed:\n",
    "    batch = reader.next_batches(3)\n",
    "    if not isinstance(batch,list):\n",
    "        proceed = False\n",
    "    else:\n",
    "        df_sum_list.append(sumBatch(batch=batch))\n",
    "        \n",
    "pl.concat(df_sum_list).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14448ed2-bbfa-407c-a1ac-c192784e0d3e",
   "metadata": {},
   "source": [
    "Add a function called `countBatch` that counts the number of rows in each batch. Only the rows without `null` values should be counted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac067ec5-4dbc-4121-9ba4-88d8316b2c4c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def countBatch(batch:list):\n",
    "    return (\n",
    "    pl.concat(\n",
    "        [\n",
    "            (\n",
    "                df\n",
    "                .select(\n",
    "                    [\n",
    "                        pl.col(\"Age\").filter(pl.col(\"Age\").is_not_null()).count(),\n",
    "                        pl.col(\"Fare\").filter(pl.col(\"Fare\").is_not_null()).count()\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "            ) for df in batch\n",
    "        ]\n",
    "    ).sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c885d704-9628-4f26-a3dc-385df0408365",
   "metadata": {},
   "source": [
    "Get the sum of all the batches in `df_sum_list` and the total count of all the batches in `df_count_list`. Concatenate all the batched to get the total sum and the total count of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adb7981-b18f-4490-8e1a-1e12315d0c79",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = pl.read_csv_batched(csvFile,batch_size=5)\n",
    "proceed = True\n",
    "df_sum_list = []\n",
    "df_count_list = []\n",
    "while proceed:\n",
    "    batch = reader.next_batches(3)\n",
    "    if not isinstance(batch,list):\n",
    "        proceed = False\n",
    "    else:\n",
    "        df_sum_list.append(sumBatch(batch=batch))\n",
    "        df_count_list.append(countBatch(batch=batch))\n",
    "pl.concat(df_sum_list).sum()\n",
    "pl.concat(df_count_list).sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b185428d-6828-4ccb-9963-2eedca95eb84",
   "metadata": {},
   "source": [
    "Divide the sum of all the batches by the total count of all the batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d1ab1a-ea87-4f9b-9221-455469d9e317",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = pl.read_csv_batched(csvFile,batch_size=5)\n",
    "proceed = True\n",
    "df_sum_list = []\n",
    "df_count_list = []\n",
    "while proceed:\n",
    "    batch = reader.next_batches(3)\n",
    "    if not isinstance(batch,list):\n",
    "        proceed = False\n",
    "    else:\n",
    "        df_sum_list.append(sumBatch(batch=batch))\n",
    "        df_count_list.append(countBatch(batch=batch))\n",
    "        \n",
    "(\n",
    "    pl.concat(df_sum_list)\n",
    "    .sum()\n",
    ") / (\n",
    "    pl.concat(df_count_list)\n",
    "    .sum()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
